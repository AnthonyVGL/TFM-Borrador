# -*- coding: utf-8 -*-
"""14MBID_TFM_Anthony_Valerio_Gomez_Lizana.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1xWqpZ7eCsGnLg08AgmYdv60cufGGY2jk

# <center>T-068 Control de Calidad Agroalimentario: Análisis Predictivo en Uvas a Diferentes Puntos de Madurez

**Nombre y apellidos:** Anthony Valerio Gomez Lizana

**Usuario VIU:** anthonyvalerio.gomez (anthonyvalerio.gomez@student.universidadviu.com)

---
# Comprensión de Datos
---

Importamos las bibliotecas a utilizar
"""

import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error
from sklearn.ensemble import RandomForestRegressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.model_selection import StratifiedShuffleSplit
from sklearn.model_selection import cross_val_score
from scipy.cluster import hierarchy
from scipy.signal import savgol_filter
from scipy import stats
from IPython.display import display

"""Se sube los archivos csv al local de google colab, donde los datos del espectrómetro y calidad se guarda con los nombres matriz_x y matriz_y respectivamente."""

# Lee los archivos CSV
matriz_x = pd.read_csv('/content/MatrizX_Uva.csv')  # Datos del espectrómetro
matriz_y = pd.read_csv('/content/MatrizY_Uva.csv', delimiter=';')  # Valores de Brix y pH

"""Se muestra el contenido del archivo matriz_y"""

# Imprimir la cantidad de filas y columnas
print(f"Cantidad de filas y columnas en matriz_y: {matriz_y.shape[0]} filas, {matriz_y.shape[1]} columnas")

# Mostrar las primeras filas del DataFrame
matriz_y.head()

"""Se modifica el nombre de la primera columna para un manejo adecuado"""

matriz_y = matriz_y.rename(columns={"º Brix": "Brix"})
print(matriz_y.shape)
matriz_y.head()

"""Se muestra el archivo matriz_x"""

# Imprimir la cantidad de filas y columnas
print(f"Cantidad de filas y columnas en matriz_x: {matriz_x.shape[0]} filas, {matriz_x.shape[1]} columnas")

# Mostrar las primeras 15 filas del DataFrame
matriz_x.head(15)

"""El dataset matriz_x necesita darle forma para una estructura adecuada, para ello se eliminará las filas con información innecesaria y se adaptará de la siguiente forma"""

# Elimina las primeras 6 filas
matriz_x = matriz_x.drop(matriz_x.index[:6])

# Se llena los valores de la primera fila con los de la segunda donde sean NaN
matriz_x.iloc[0] = matriz_x.iloc[0].combine_first(matriz_x.iloc[1])

# Se elimina la fila con indice 1
matriz_x = matriz_x.drop(matriz_x.index[1])
matriz_x = matriz_x.drop(matriz_x.index[1])

# Elimina la primera columna
matriz_x = matriz_x.drop(matriz_x.columns[0], axis=1)

# Elimina la última columna
matriz_x = matriz_x.drop(matriz_x.columns[-1], axis=1)

# Restablece la fila 7 (índice 0) como los títulos de las columnas
matriz_x.columns = matriz_x.iloc[0]

# Resetea los índices para evitar saltos en la numeración
matriz_x = matriz_x.reset_index(drop=True)

# Elimina la fila duplicada en el indice 0
matriz_x = matriz_x.drop(matriz_x.index[0])

# Verifica el resultado
matriz_x.head()

"""Teniendo la estructura adecuada observamos que entre las variables esta el analysis date, esta variable se modificará el formato para obtener dos columnas extras una de fecha y otra que contabilizará el número de días según la columna Sample teniendo como referencia siempre la primera Fecha a contabilizar y las demás del mismo Sample se contarán los días."""

# Función para agregar el 0 faltante al año
def corregir_fecha(fecha):
    if fecha.startswith('021'):
        fecha = '2021' + fecha[3:]
    return fecha

# Aplicar la corrección a la columna 'analysis date'
matriz_x['analysis date corregido'] = matriz_x['analysis date'].apply(corregir_fecha)

# Convertir la columna corregida a datetime
matriz_x['analysis date corregido'] = pd.to_datetime(matriz_x['analysis date corregido'], format='%Y-%m-%dT%H-%M-%S')

# Convertir al formato deseado dd/mm/yyyy
matriz_x['Fecha'] = matriz_x['analysis date corregido'].dt.strftime('%d/%m/%Y')

# Eliminar la columna 'analysis date corregido'
matriz_x = matriz_x.drop(columns=["analysis date corregido"])

# Eliminar las columnas no necesarias
matriz_x = matriz_x.drop(columns=["sample name", "Time Index", "Replicate"])

# Mover la columna 'Fecha' al lado de 'analysis date'
matriz_x.insert(2, "Fecha", matriz_x.pop("Fecha"))

# Asegurarnos de que la columna 'Fecha' está en formato datetime
matriz_x['Fecha'] = pd.to_datetime(matriz_x['Fecha'], format='%d/%m/%Y')

# Calcular la columna 'Días' tomando la primera fecha de cada muestra como referencia
matriz_x['Días'] = matriz_x.groupby('Sample')['Fecha'].transform(lambda x: (x - x.min()).dt.days)

# Insertar la columna "Días" en la posición 3
matriz_x.insert(3, "Días", matriz_x.pop("Días"))

# Renombrar la columna "type of fermentation" a "Tipo de uva"
matriz_x.rename(columns={"type of fermentation": "Tipo de uva"}, inplace=True)

# Mostrar las primeras filas para verificar
matriz_x.head()

"""Ahora se elimina las columnas que no aportarán valor como el analysis date, Fecha y Subfile Index"""

# Eliminar las columnas 'analysis date', 'Subfile Index', 'Fecha' y 'Sampling time'
matriz_x = matriz_x.drop(columns=['analysis date', 'Subfile Index', 'Fecha', 'Sampling time'])

# Reiniciar el índice
matriz_x.reset_index(drop=True, inplace=True)

# Verificar que las columnas se hayan eliminado correctamente
matriz_x.head()

"""Ahora el dataset matriz_x contiene las variables y una estructura adecuadas para el análisis, además de estar compuesta por las siguientes variables
*   Full name = Codificación al unir Sample, Samplin time, Replicate y analysis date
*   Días = La cantidad de días de cada Sample considerando la primera fecha como 0.
*   Sample = Es la muestra tomada del tipo de uva siendo la primera o segunda muestra con dichas características.
*   Tipo de uva = Tipo de uva con las siguientes nomenclaturas:

  M = Variedad, A=Arriba, M=Medio y B=Bajo.

  La primera letra M es de variedad Moscatel de Alejandría.

  La segunda letra indica la posición de la Viña en donde se tiene la medida de la uva sea A, M o B.

  La tercera letra indica la posición de racismo de uva dentro de la planta sea A, M o B.

A partir de la columna Sample se muestra los distintos números de onda (cm-1)

Identificación de valores nulos o faltantes de cada matriz
"""

# Para matriz_x
nulos_x = matriz_x.isnull().sum()
print("Valores nulos en matriz_x:")
print(nulos_x[nulos_x > 0])

# Para matriz_y
if isinstance(matriz_y, pd.DataFrame):
    nulos_y = matriz_y.isnull().sum()
    print("Valores nulos en matriz_y:")
    print(nulos_y[nulos_y > 0])

"""Se observa en el resultado es 0 en cada matriz por lo que los datasets tienen completa su información

Ahora identificaremos los outliers de la matriz_y
"""

# Configurar el estilo de los gráficos
sns.set(style="whitegrid")

# Crear una figura con dos gráficos de boxplot (uno para cada variable)
fig, ax = plt.subplots(1, 2, figsize=(12, 6))

# Graficar boxplot para la columna 'Brix'
sns.boxplot(data=matriz_y['Brix'], ax=ax[0])
ax[0].set_title('Boxplot de Brix')
ax[0].set_ylabel('Valor de Brix')

# Graficar boxplot para la columna 'pH'
sns.boxplot(data=matriz_y['pH'], ax=ax[1])
ax[1].set_title('Boxplot de pH')
ax[1].set_ylabel('Valor de pH')

# Mostrar los gráficos
plt.tight_layout()
plt.show()

# Identificar y mostrar los valores de outliers en cada columna
def identificar_outliers(df, columna):
    Q1 = df[columna].quantile(0.25)  # Primer cuartil
    Q3 = df[columna].quantile(0.75)  # Tercer cuartil
    IQR = Q3 - Q1  # Rango intercuartílico

    # Limites para identificar outliers
    limite_inferior = Q1 - 1.5 * IQR
    limite_superior = Q3 + 1.5 * IQR

    # Filtrar outliers
    outliers = df[(df[columna] < limite_inferior) | (df[columna] > limite_superior)]
    return outliers[columna]

def identificar_outliers_index(df, columna):
    Q1 = df[columna].quantile(0.25)  # Primer cuartil
    Q3 = df[columna].quantile(0.75)  # Tercer cuartil
    IQR = Q3 - Q1  # Rango intercuartílico

    # Limites para identificar outliers
    limite_inferior = Q1 - 1.5 * IQR
    limite_superior = Q3 + 1.5 * IQR

    # Filtrar outliers
    outliers_indices = df[(df[columna] < limite_inferior) | (df[columna] > limite_superior)].index
    return outliers_indices

# Identificar y mostrar outliers
outliers_brix = identificar_outliers(matriz_y, 'Brix')
outliers_ph = identificar_outliers(matriz_y, 'pH')

# Identificar los indices de los outliers en Brix y pH
indices_outliers_brix = identificar_outliers_index(matriz_y, 'Brix')
indices_outliers_ph = identificar_outliers_index(matriz_y, 'pH')

print("Outliers en la columna Brix:", outliers_brix)

print("\nOutliers en la columna pH:", outliers_ph)

"""Se visualizar que existe 1 caso de outliers en la gráfica de boxplot para la columnas Brix y pH, siendo los valores 12.9 en la posición 9 y 2.87 en la posición 0 respectivamente.

Los resultados de los outliers se encuentran dentro de los valores establecidos en los indicadores de Brix y pH, por lo que se conservarán para que puedan aportar información útil al modelo.

Ahora se grafica las muestras en todos los espectros para analizar la matriz_x
"""

espectros = matriz_x.iloc[:, 7:]  # Todas las filas y columnas a partir de la 7
numeros_onda = matriz_x.columns[7:]  # Los nombres de las columnas de números de onda

# Graficamos los espectros de todas las filas
plt.figure(figsize=(12, 8))

# Iteramos sobre todas las filas
for i in range(espectros.shape[0]):
    plt.plot(numeros_onda, espectros.iloc[i, :], label=f'Fila {i+1}')  # Graficamos cada fila

# Configuramos el gráfico
plt.title('Espectros de todas las filas de matriz_x')
plt.xlabel('Números de onda (cm-1)')
plt.ylabel('Intensidad')
plt.grid(True)
plt.tight_layout()
plt.show()

"""Se observa que los valores no se encuentran tan distantes entre sí, lo que da indicios a que no exista valores de outliers en la matriz_x, la diferencia más notoría se centra entre los números de onda entre (900, 1500) y (3200, 3400), se realizará el gráfico en ese rango para visualizar mejor el comportamiento de los datos."""

# Filtramos los números de onda en los rangos deseados
rango_espectros_1 = (numeros_onda >= 900) & (numeros_onda <= 1500)
rango_espectros_2 = (numeros_onda >= 3200) & (numeros_onda <= 3400)

# Espectros filtrados para cada rango
espectros_filtrados_1 = espectros.loc[:, rango_espectros_1]
espectros_filtrados_2 = espectros.loc[:, rango_espectros_2]

numeros_onda_filtrados_1 = numeros_onda[rango_espectros_1]  # Números de onda filtrados para el primer rango
numeros_onda_filtrados_2 = numeros_onda[rango_espectros_2]  # Números de onda filtrados para el segundo rango

# Graficamos los espectros de todas las filas en el primer rango
plt.figure(figsize=(12, 6))
for i in range(espectros_filtrados_1.shape[0]):
    plt.plot(numeros_onda_filtrados_1, espectros_filtrados_1.iloc[i, :], label=f'Fila {i+1}')  # Graficamos cada fila
plt.title('Espectros de todas las filas de matriz_x (900-1500 cm-1)')
plt.xlabel('Números de onda (cm-1)')
plt.ylabel('Intensidad')
plt.grid(True)
plt.tight_layout()
plt.show()

# Graficamos los espectros de todas las filas en el segundo rango
plt.figure(figsize=(12, 6))
for i in range(espectros_filtrados_2.shape[0]):
    plt.plot(numeros_onda_filtrados_2, espectros_filtrados_2.iloc[i, :], label=f'Fila {i+1}')  # Graficamos cada fila
plt.title('Espectros de todas las filas de matriz_x (3200-3400 cm-1)')
plt.xlabel('Números de onda (cm-1)')
plt.ylabel('Intensidad')
plt.grid(True)
plt.tight_layout()
plt.show()

"""Todas las muestran siguen el mismo patrón, casi ni parece cruzarse entre sí, es un indicio de que en estos niveles de números de onda captan mayor la variabilidad de las muestras entre sí.

----
# Selección de Variables
----

Se tiene como finalidad realizar los modelos con los datos de la espectroscopia, pero para un mayor nivel de entrenamiento se realizará las siguiente prueba para considerar nuevas variables de los datos.

Ahora se graficará un mapa de calor considerando solo las variables de Horas y tipo de uva en relación al Brix y pH, para identificar si guardan algún tipo de relación y poder implementarlas al modelo de Machine Learning
"""

# Preparar los datos a partir de 'matriz_x' (Días, Tipo de uva)
X_datos = matriz_x[['Días', 'Tipo de uva']].copy()

# Extraer los caracteres específicos de 'Tipo de uva'
X_datos['Viña'] = X_datos['Tipo de uva'].str[-2]  # Segundo carácter desde el final (representa Viña)
X_datos['Racimo'] = X_datos['Tipo de uva'].str[-1]  # Último carácter (representa Racimo)

# Convertir las categorías de 'Viña' y 'Racimo'' en variables dummy
X_datos = pd.get_dummies(X_datos, columns=['Viña', 'Racimo'], drop_first=False)

# Eliminar las columnas originales no numéricas 'Tipo de uva'
X_datos = X_datos.drop(columns=['Tipo de uva'])

# Agregar las variables objetivo (Brix y pH) desde 'matriz_y'
X_datos['Brix'] = matriz_y['Brix']
X_datos['pH'] = matriz_y['pH']

# Calcular la matriz de correlación
correlacion = X_datos.corr()

# Visualizar el mapa de calor
plt.figure(figsize=(10, 8))
sns.heatmap(correlacion, annot=True, cmap='coolwarm', fmt='.2f', linewidths=0.5)
plt.title('Mapa de calor de correlaciones entre Días,Tipo de uva, Brix y pH')
plt.show()

"""Resultados del mapa de calor demuestran los siguiente:

Para el Brix la variable más significativa es Días siguiendo de Viña_B (Posición de la Viña Baja) y Viña_A (Posición de la viña Alta).

Para el pH la variable más significativa es Días, siguiendo de Racimo_B (Posición del racimo Bajo) y Racimo_A (Posición del racimo Alto).

Existe una relación significativa entre las variables a predecir entre sí el Brix y el pH siendo estos valores de Matriz_y.

Por lo tanto, se concluye que las nuevas variables de entrada a sumar de la espectroscopía serán los Días y Tipo de uva para el entrenamiento de los modelos.

---
# Primera iteración
---

---
##Entrenamiento
---

Se plantea usar todos los datos posibles que tengan algún tipo de relación con los valores a predecir Brix y pH.

Para el tratamiento de preparar los datos se usarán todas las variables de espectros y se considerará las variables Días y Tipo de uva.
"""

# Preparar los datos
X_completo = pd.concat([espectros, matriz_x[['Días', 'Tipo de uva']]], axis=1)

# Extraer los caracteres específicos de 'Tipo de uva' para crear 'Viña' y 'Racimo'
X_completo['Viña'] = X_completo['Tipo de uva'].str[-2]  # Segundo carácter desde el final (representa Viña)
X_completo['Racimo'] = X_completo['Tipo de uva'].str[-1]  # Último carácter (representa Racimo)

# Codificación de variables categóricas ('Viña' y 'Racimo')
X_completo = pd.get_dummies(X_completo, columns=['Viña', 'Racimo'], drop_first=False)

# Eliminar las columnas originales no numéricas 'Tipo de uva'
X_completo = X_completo.drop(columns=['Tipo de uva'])

# Normalizar solo la columna 'Días'
scaler = StandardScaler()

# Normalizamos solo la columna 'Días'
X_completo[['Días']] = scaler.fit_transform(X_completo[['Días']])
X_completo.columns = X_completo.columns.astype(str)

# Mostrar las primeras filas para verificar
X_completo.head()

"""Al aplicar pd.get_dummies() para convertir la columna Tipo de uva en variables dummy, la columna original se convierte en varias columnas binarias, cada una representando la posición según la viña y del racimo, siendo A=Alto, B=Bajo y M=Medio.

Se realiza la separación de los datos para el entrenamiento y el test en un 70% y 30% ya que es el que mejor resultados da.
"""

# Separar en conjunto de entrenamiento para Brix
y_brix = matriz_y.iloc[:, 0].values  # Seleccionar solo la columna de Brix
y_ph = matriz_y.iloc[:, 1].values  # Seleccionar solo la columna de pH

# Usar train_test_split para dividir los datos aleatoriamente
X_train, X_test, y_train, y_test = train_test_split(X_completo, y_brix, test_size=0.3, random_state=42)
# Usar train_test_split para dividir los datos de Brix y pH
X_train_ph, X_test_ph, y_train_ph, y_test_ph = train_test_split(X_completo, y_ph, test_size=0.3, random_state=42)

"""----
##Modelado para Brix
----

En esta sección de Predicción Brix se ha decidido entrenar distintos modelos de Machine Learning para poder seleccionar el que da mejores resultados con nuestros datos en la predicción del Brix.

**Regresión Lineal**
"""

# Inicializar el modelo para Brix
lin_reg = LinearRegression()

# Ajustar el modelo para Brix
lin_reg.fit(X_train, y_train)

# Realizar predicciones para Brix
y_pred_lin = lin_reg.predict(X_test)

# Evaluar el modelo para Brix
mse_lin = mean_squared_error(y_test, y_pred_lin)
mae_lin = mean_absolute_error(y_test, y_pred_lin)
r2_lin = r2_score(y_test, y_pred_lin)

print("Regresión Lineal para Brix")
print(f"Mean Squared Error (MSE): {mse_lin}")
print(f"Mean Absolute Error (MAE): {mae_lin}")
print(f"R-squared: {r2_lin}")

"""**Árbol de Decisión**"""

# Inicializar el modelo
tree_reg = DecisionTreeRegressor()

# Ajustar el modelo
tree_reg.fit(X_train, y_train)

# Realizar predicciones
y_pred_tree = tree_reg.predict(X_test)

# Evaluar el modelo
mse_tree = mean_squared_error(y_test, y_pred_tree)
mae_tree = mean_absolute_error(y_test, y_pred_tree)
r2_tree = r2_score(y_test, y_pred_tree)

print("Árbol de Decisión")
print(f"Mean Squared Error (MSE): {mse_tree}")
print(f"Mean Absolute Error (MAE): {mae_tree}")
print(f"R-squared: {r2_tree}\n")

"""**Random Forest**"""

# Inicializar el modelo
rf_reg = RandomForestRegressor()

# Ajustar el modelo
rf_reg.fit(X_train, y_train)

# Realizar predicciones
y_pred_rf = rf_reg.predict(X_test)

# Evaluar el modelo
mse_rf = mean_squared_error(y_test, y_pred_rf)
mae_rf = mean_absolute_error(y_test, y_pred_rf)
r2_rf = r2_score(y_test, y_pred_rf)

print("Random Forest")
print(f"Mean Squared Error (MSE): {mse_rf}")
print(f"Mean Absolute Error (MAE): {mae_rf}")
print(f"R-squared: {r2_rf}\n")

"""###Evaluación Modelos Brix"""

# Evaluación Modelos Brix
resultados = pd.DataFrame({
    'Modelo': ['Regresión Lineal', 'Árbol de Decisión', 'Random Forest'],
    'MSE': [mse_lin, mse_tree, mse_rf],
    'MAE': [mae_lin, mae_tree, mae_rf],
    'R-squared': [r2_lin, r2_tree, r2_rf]
})

print(resultados)

"""----
##Modelado para pH
----

**Regresión Lineal**
"""

# Inicializar el modelo de regresión lineal
lin_reg_brix_ph = LinearRegression()

# Ajustar el modelo usando los datos de entrenamiento
lin_reg_brix_ph.fit(X_train_ph, y_train_ph)

# Realizar predicciones para el conjunto de prueba
y_pred_ph = lin_reg_brix_ph.predict(X_test_ph)

# Evaluar el modelo para predecir pH
mse_ph_lin = mean_squared_error(y_test_ph, y_pred_ph)
mae_ph_lin = mean_absolute_error(y_test_ph, y_pred_ph)
r2_ph_lin = r2_score(y_test_ph, y_pred_ph)

print("Modelo de Regresión Lineal para pH")
print(f"Mean Squared Error (MSE): {mse_ph_lin}")
print(f"Mean Absolute Error (MAE): {mae_ph_lin}")
print(f"R-squared: {r2_ph_lin}\n")

"""**Árbol de Decisión**"""

# Inicializar el modelo de Árbol de Decisión
tree_reg_brix_ph = DecisionTreeRegressor(random_state=42)

# Ajustar el modelo usando los datos de entrenamiento (Brix como entrada, pH como salida)
tree_reg_brix_ph.fit(X_train_ph, y_train_ph)

# Realizar predicciones para el conjunto de prueba
y_pred_ph_tree = tree_reg_brix_ph.predict(X_test_ph)

# Evaluar el modelo para predecir pH
mse_ph_tree = mean_squared_error(y_test_ph, y_pred_ph_tree)
mae_ph_tree = mean_absolute_error(y_test_ph, y_pred_ph_tree)
r2_ph_tree = r2_score(y_test_ph, y_pred_ph_tree)

print("Árbol de Decisión para pH:")
print(f"Mean Squared Error (MSE): {mse_ph_tree}")
print(f"Mean Absolute Error (MAE): {mae_ph_tree}")
print(f"R-squared: {r2_ph_tree}\n")

"""**Random Forest**"""

# Inicializar el modelo de Random Forest
rf_reg_brix_ph = RandomForestRegressor(n_estimators=200, random_state=42)

# Ajustar el modelo usando los datos de entrenamiento
rf_reg_brix_ph.fit(X_train_ph, y_train_ph)

# Realizar predicciones para el conjunto de prueba
y_pred_ph_rf = rf_reg_brix_ph.predict(X_test_ph)

# Evaluar el modelo para predecir pH
mse_ph_rf = mean_squared_error(y_test_ph, y_pred_ph_rf)
mae_ph_rf = mean_absolute_error(y_test_ph, y_pred_ph_rf)
r2_ph_rf = r2_score(y_test_ph, y_pred_ph_rf)

# Imprimir los resultados
print("Random Forest para pH:")
print(f"Mean Squared Error (MSE): {mse_ph_rf}")
print(f"Mean Absolute Error (MAE): {mae_ph_rf}")
print(f"R-squared: {r2_ph_rf}\n")

"""###Evaluación Modelos pH"""

# Evaluación Modelos pH
resultados = pd.DataFrame({
    'Modelo': ['Regresión Lineal', 'Árbol de Decisión', 'Random Forest'],
    'MSE': [mse_ph_lin, mse_ph_tree, mse_ph_rf],
    'MAE': [mae_ph_lin, mae_ph_tree, mae_ph_rf],
    'R-squared': [r2_ph_lin, r2_ph_tree, r2_ph_rf]
})

# Imprimir los resultados
print(resultados)

"""---
# Segunda iteración
---

Se realiza la preparación de datos sin pretratamiento para PCA.

###Análisis de Componentes Principales (PCA) sin Tratamiento
"""

# Número de componentes a 2 para visualización en 2D
n_componentes = 2
pca = PCA(n_components=n_componentes)
componentes_principales_s = pca.fit_transform(espectros)

# Graficar en 2D si n_components=2
if n_componentes == 2:
    plt.figure(figsize=(10, 6))
    plt.scatter(componentes_principales_s[:, 0], componentes_principales_s[:, 1], alpha=0.7)
    plt.title('PCA - Componentes Principales en 2D')
    plt.xlabel('Componente Principal 1')
    plt.ylabel('Componente Principal 2')
    plt.grid()
    plt.show()

# Si deseas ver la varianza explicada por cada componente:
print("Varianza explicada por cada componente:", pca.explained_variance_ratio_)

"""---
## Entrenamiento con PCA sin tratamiento
---

Se plantea usar todos los datos posibles que tengan algún tipo de relación con los valores a predecir Brix y pH.

Para el entrenamiento de preparar los datos se usarán todas las variables de espectros y se considerará las variables Días y Tipo de uva.
"""

# Crear un DataFrame con los componentes principales
df_componentes_principales = pd.DataFrame(componentes_principales_s, columns=['Componente 1', 'Componente 2'])

# Asegurarse de que matriz_x tenga un índice correcto
matriz_x.reset_index(drop=True, inplace=True)  # Reiniciar el índice si es necesario

# Agregar las columnas 'Días' y 'Tipo de uva' a los componentes principales
df_componentes_principales['Días'] = matriz_x['Días'].reset_index(drop=True)  # Asegurarse de que las longitudes coincidan
df_componentes_principales['Tipo de uva'] = matriz_x['Tipo de uva'].reset_index(drop=True)  # Asegurarse de que las longitudes coincidan

# Separar 'Tipo de uva' en dos nuevas columnas 'Racimo' y 'Viña'
df_componentes_principales['Viña'] = df_componentes_principales['Tipo de uva'].str[1]  # Segundo carácter
df_componentes_principales['Racimo'] = df_componentes_principales['Tipo de uva'].str[2]  # Tercer carácter

# Eliminar la columna 'Tipo de uva'
df_componentes_principales.drop(columns=['Tipo de uva'], inplace=True)

# Codificación de variables categóricas ('Viña' y 'Racimo')
df_componentes_principales = pd.get_dummies(df_componentes_principales, columns=['Viña', 'Racimo'], drop_first=False)

# Normalizar la columna 'Días'
scaler = StandardScaler()
df_componentes_principales[['Días']] = scaler.fit_transform(df_componentes_principales[['Días']])

# Mostrar el DataFrame resultante
print(df_componentes_principales.head())

"""Al aplicar pd.get_dummies() para convertir la columna Tipo de uva en variables dummy, la columna original se convierte en varias columnas binarias, cada una representando la posición según la viña y del racimo, siendo A=Alto, B=Bajo y M=Medio.

Se realiza la separación de los datos para el entrenamiento y el test en un 70% y 30% ya que es el que mejor resultados da.
"""

# Usar el DataFrame con los componentes principales (df_componentes_principales)
X_train_brix_pca, X_test_brix_pca, y_train_brix_pca, y_test_brix_pca = train_test_split(df_componentes_principales, y_brix, test_size=0.3, random_state=42)
X_train_ph_pca, X_test_ph_pca, y_train_ph_pca, y_test_ph_pca = train_test_split(df_componentes_principales, y_ph, test_size=0.3, random_state=42)

"""----
##Modelado para Brix con PCA sin tratamiento
----

En esta sección de Predicción Brix se ha decidido entrenar distintos modelos de Machine Learning para poder seleccionar el que da mejores resultados con nuestros datos en la predicción del Brix.

**Regresión Lineal**
"""

# Inicializar el modelo para Brix
lin_reg_pca = LinearRegression()

# Ajustar el modelo para Brix
lin_reg_pca.fit(X_train_brix_pca, y_train_brix_pca)

# Realizar predicciones para Brix
y_pred_lin_pca = lin_reg_pca.predict(X_test_brix_pca)

# Evaluar el modelo para Brix
mse_lin_pca = mean_squared_error(y_test_brix_pca, y_pred_lin_pca)
mae_lin_pca = mean_absolute_error(y_test_brix_pca, y_pred_lin_pca)
r2_lin_pca = r2_score(y_test_brix_pca, y_pred_lin_pca)

print("Regresión Lineal para Brix con PCA")
print(f"Mean Squared Error (MSE): {mse_lin_pca}")
print(f"Mean Absolute Error (MAE): {mae_lin_pca}")
print(f"R-squared: {r2_lin_pca}\n")

"""**Árbol de Decisión**"""

# Inicializar el modelo
tree_reg_pca = DecisionTreeRegressor()

# Ajustar el modelo
tree_reg_pca.fit(X_train_brix_pca, y_train_brix_pca)

# Realizar predicciones
y_pred_tree_pca = tree_reg_pca.predict(X_test_brix_pca)

# Evaluar el modelo
mse_tree_pca = mean_squared_error(y_test_brix_pca, y_pred_tree_pca)
mae_tree_pca = mean_absolute_error(y_test_brix_pca, y_pred_tree_pca)
r2_tree_pca = r2_score(y_test_brix_pca, y_pred_tree_pca)

print("Árbol de Decisión para Brix con PCA")
print(f"Mean Squared Error (MSE): {mse_tree_pca}")
print(f"Mean Absolute Error (MAE): {mae_tree_pca}")
print(f"R-squared: {r2_tree_pca}\n")

"""**Random Forest**"""

# Inicializar el modelo
rf_reg_pca = RandomForestRegressor()

# Ajustar el modelo
rf_reg_pca.fit(X_train_brix_pca, y_train_brix_pca)

# Realizar predicciones
y_pred_rf_pca = rf_reg_pca.predict(X_test_brix_pca)

# Evaluar el modelo
mse_rf_pca = mean_squared_error(y_test_brix_pca, y_pred_rf_pca)
mae_rf_pca = mean_absolute_error(y_test_brix_pca, y_pred_rf_pca)
r2_rf_pca = r2_score(y_test_brix_pca, y_pred_rf_pca)

print("Random Forest para Brix con PCA")
print(f"Mean Squared Error (MSE): {mse_rf_pca}")
print(f"Mean Absolute Error (MAE): {mae_rf_pca}")
print(f"R-squared: {r2_rf_pca}\n")

"""### Evaluación Modelos Brix con PCA sin tratamiento"""

# Evaluación Modelos Brix con PCA sin tratamiento
resultados_pca = pd.DataFrame({
    'Modelo': ['Regresión Lineal', 'Árbol de Decisión', 'Random Forest'],
    'MSE': [mse_lin_pca, mse_tree_pca, mse_rf_pca],
    'MAE': [mae_lin_pca, mae_tree_pca, mae_rf_pca],
    'R-squared': [r2_lin_pca, r2_tree_pca, r2_rf_pca]
})

print(resultados_pca)

"""----
##Modelado para pH con PCA sin tratamiento
----

**Regresión Lineal**
"""

# Inicializar el modelo de regresión lineal
lin_reg_brix_ph = LinearRegression()

# Ajustar el modelo usando los datos de entrenamiento
lin_reg_brix_ph.fit(X_train_ph_pca, y_train_ph_pca)

# Realizar predicciones para el conjunto de prueba
y_pred_ph_pca = lin_reg_brix_ph.predict(X_test_ph_pca)

# Evaluar el modelo para predecir pH
mse_ph_lin_pca = mean_squared_error(y_test_ph_pca, y_pred_ph_pca)
mae_ph_lin_pca = mean_absolute_error(y_test_ph_pca, y_pred_ph_pca)
r2_ph_lin_pca = r2_score(y_test_ph_pca, y_pred_ph_pca)

print("Modelo de Regresión Lineal:")
print(f"Mean Squared Error: {mse_ph_lin_pca}")
print(f"Mean Absolute Error: {mae_ph_lin_pca}")
print(f"R-squared: {r2_ph_lin_pca}\n")

"""**Árbol de Decisión**"""

# Inicializar el modelo de Árbol de Decisión
tree_reg_brix_ph = DecisionTreeRegressor(random_state=42)

# Ajustar el modelo usando los datos de entrenamiento (Brix como entrada, pH como salida)
tree_reg_brix_ph.fit(X_train_ph_pca, y_train_ph_pca)

# Realizar predicciones para el conjunto de prueba
y_pred_ph_tree_pca = tree_reg_brix_ph.predict(X_test_ph_pca)

# Evaluar el modelo para predecir pH
mse_ph_tree_pca = mean_squared_error(y_test_ph_pca, y_pred_ph_tree_pca)
mae_ph_tree_pca = mean_absolute_error(y_test_ph_pca, y_pred_ph_tree_pca)
r2_ph_tree_pca = r2_score(y_test_ph_pca, y_pred_ph_tree_pca)

print("Árbol de Decisión:")
print(f"Mean Squared Error: {mse_ph_tree_pca}")
print(f"Mean Absolute Error: {mae_ph_tree_pca}")
print(f"R-squared: {r2_ph_tree_pca}\n")

"""**Random Forest**"""

# Inicializar el modelo de Random Forest
rf_reg_brix_ph = RandomForestRegressor(n_estimators=100, random_state=42)

# Ajustar el modelo usando los datos de entrenamiento
rf_reg_brix_ph.fit(X_train_ph_pca, y_train_ph_pca)

# Realizar predicciones para el conjunto de prueba
y_pred_ph_rf_pca = rf_reg_brix_ph.predict(X_test_ph_pca)

# Evaluar el modelo para predecir pH
mse_ph_rf_pca = mean_squared_error(y_test_ph_pca, y_pred_ph_rf_pca)
mae_ph_rf_pca = mean_absolute_error(y_test_ph_pca, y_pred_ph_rf_pca)
r2_ph_rf_pca = r2_score(y_test_ph_pca, y_pred_ph_rf_pca)

print("Random Forest:")
print(f"Mean Squared Error: {mse_ph_rf_pca}")
print(f"Mean Absolute Error: {mae_ph_rf_pca}")
print(f"R-squared: {r2_ph_rf_pca}\n")

"""###Evaluación Modelos pH con PCA sin tratamiento"""

#Evaluación Modelos pH con PCA sin tratamiento
resultados = pd.DataFrame({
    'Modelo': ['Regresión Lineal', 'Árbol de Decisión', 'Random Forest'],
    'MSE': [mse_ph_lin_pca, mse_ph_tree_pca, mse_ph_rf_pca],
    'MAE': [mae_ph_lin_pca, mae_ph_tree_pca, mae_ph_rf_pca],
    'R-squared': [r2_ph_lin_pca, r2_ph_tree_pca, r2_ph_rf_pca]
})

print(resultados)

"""---
# Tercera iteración
---

Se realiza la preparación de datos con pretratamiento para PCA.

---
##Preparación de Datos PCA
---

Se realiza los siguientes pasos de pretratamiento para realizar un PCA

###Suavizado
"""

# Suavizar los datos espectrales
window_length = 9  # Impar
polyorder = 2      # Grado del polinomio para el suavizado
espectros_suavizados = savgol_filter(espectros, window_length=window_length, polyorder=polyorder, axis=1)

# Convertir a DataFrame para facilitar la manipulación
espectros_suavizados_df = pd.DataFrame(espectros_suavizados, columns=numeros_onda)

fila_a_graficar = 0

plt.figure(figsize=(12, 6))

# Graficar el espectro original filtrado
plt.plot(numeros_onda_filtrados_1, espectros_filtrados_1.iloc[fila_a_graficar, :],
         label='Original', color='red', alpha=0.5)

# Graficar el espectro suavizado y filtrado
plt.plot(numeros_onda_filtrados_1, espectros_suavizados_df.loc[fila_a_graficar, rango_espectros_1],
         label='Suavizado', linewidth=2, color='blue')

# Configuraciones del gráfico
plt.title(f'Comparación de Espectro Original y Suavizado - Fila {fila_a_graficar + 1}')
plt.xlabel('Números de onda (cm-1)')
plt.ylabel('Intensidad')
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

"""###Primera Derivada"""

# Aplicar la primera derivada al espectro suavizado
derivada_1 = savgol_filter(espectros_suavizados, window_length=window_length, polyorder=polyorder, deriv=1, axis=1)

# Convertir la derivada a DataFrame para facilitar la manipulación
derivada_1_df = pd.DataFrame(derivada_1, columns=numeros_onda)

# Fila a graficar
fila_a_graficar = 0

# Graficar el espectro original, suavizado y la primera derivada
plt.figure(figsize=(12, 6))

# Graficar el espectro suavizado
plt.plot(numeros_onda, espectros_suavizados_df.iloc[fila_a_graficar, :], label='Suavizado', linewidth=2, color='blue')

# Graficar la primera derivada
plt.plot(numeros_onda, derivada_1_df.iloc[fila_a_graficar, :], label='Primera Derivada', linewidth=2, color='green')

# Configuración del gráfico
plt.title(f'Comparación: Espectro Suavizado y Primera Derivada - Fila {fila_a_graficar + 1}')
plt.xlabel('Números de onda (cm-1)')
plt.ylabel('Intensidad')
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

"""###Normalización"""

# Crear un objeto Z-score para normalizar entre 0 y 1
scaler = StandardScaler()

# Normalizar la primera derivada
derivada_1_normalizada = scaler.fit_transform(derivada_1_df)

# Convertir a DataFrame para facilitar la manipulación
derivada_1_normalizada_df = pd.DataFrame(derivada_1_normalizada, columns=numeros_onda)

# Graficar el espectro normalizado de la primera derivada
plt.figure(figsize=(12, 6))

# Graficar la primera derivada normalizada
plt.plot(numeros_onda, derivada_1_normalizada_df.iloc[fila_a_graficar, :], label='Primera Derivada Normalizada', color='purple', linewidth=2)

# Graficar la primera derivada
plt.plot(numeros_onda, derivada_1_df.iloc[fila_a_graficar, :], label='Primera Derivada', linewidth=2, color='green')

# Configuración del gráfico
plt.title(f'Espectro de la Primera Derivada Normalizado - Fila {fila_a_graficar + 1}')
plt.xlabel('Números de onda (cm-1)')
plt.ylabel('Intensidad Normalizada')
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

"""###Análisis de Componentes Principales (PCA)"""

# Aplicar PCA a todos los espectros SNV
n_componentes = 2
pca = PCA(n_components=n_componentes)

# Aplicar PCA a la matriz de espectros SNV
componentes_principales = pca.fit_transform(derivada_1_normalizada)

# Graficar los componentes principales en 2D
if n_componentes == 2:
    plt.figure(figsize=(10, 6))
    plt.scatter(componentes_principales[:, 0], componentes_principales[:, 1], alpha=0.7)
    plt.title('PCA - Componentes Principales en 2D')
    plt.xlabel('Componente Principal 1')
    plt.ylabel('Componente Principal 2')
    plt.grid(True)
    plt.show()

# Imprimir la varianza explicada por cada componente principal
print("Varianza explicada por cada componente:", pca.explained_variance_ratio_)

"""---
## Entrenamiento con PCA
---

Se plantea usar todos los datos posibles que tengan algún tipo de relación con los valores a predecir Brix y pH.

Para el entrenamiento de preparar los datos se usarán todas las variables de espectros y se considerará las variables Días y Tipo de uva.
"""

# Crear un DataFrame con los componentes principales
df_componentes_principales_pca = pd.DataFrame(componentes_principales, columns=['Componente 1', 'Componente 2'])

# Asegurarse de que matriz_x tenga un índice correcto
matriz_x.reset_index(drop=True, inplace=True)  # Reiniciar el índice si es necesario

# Agregar las columnas 'Días' y 'Tipo de uva' a los componentes principales
df_componentes_principales_pca['Días'] = matriz_x['Días'].reset_index(drop=True)  # Asegurarse de que las longitudes coincidan
df_componentes_principales_pca['Tipo de uva'] = matriz_x['Tipo de uva'].reset_index(drop=True)  # Asegurarse de que las longitudes coincidan

# Separar 'Tipo de uva' en dos nuevas columnas 'Racimo' y 'Viña'
df_componentes_principales_pca['Viña'] = df_componentes_principales_pca['Tipo de uva'].str[1]  # Segundo carácter
df_componentes_principales_pca['Racimo'] = df_componentes_principales_pca['Tipo de uva'].str[2]  # Tercer carácter

# Eliminar la columna 'Tipo de uva'
df_componentes_principales_pca.drop(columns=['Tipo de uva'], inplace=True)

# Codificación de variables categóricas ('Viña' y 'Racimo')
df_componentes_principales_pca = pd.get_dummies(df_componentes_principales_pca, columns=['Viña', 'Racimo'], drop_first=False)

# Normalizar la columna 'Días'
scaler = StandardScaler()
df_componentes_principales_pca[['Días']] = scaler.fit_transform(df_componentes_principales_pca[['Días']])

# Mostrar el DataFrame resultante
print(df_componentes_principales_pca.head())

"""Al aplicar pd.get_dummies() para convertir la columna Tipo de uva en variables dummy, la columna original se convierte en varias columnas binarias, cada una representando la posición según la viña y del racimo, siendo A=Alto, B=Bajo y M=Medio.

Se realiza la separación de los datos para el entrenamiento y el test en un 70% y 30% ya que es el que mejor resultados da.
"""

# Usar el DataFrame con los componentes principales (df_componentes_principales)
X_train_brix_pca_t, X_test_brix_pca_t, y_train_brix_pca_t, y_test_brix_pca_t = train_test_split(df_componentes_principales_pca, y_brix, test_size=0.3, random_state=42)
X_train_ph_pca_t, X_test_ph_pca_t, y_train_ph_pca_t, y_test_ph_pca_t = train_test_split(df_componentes_principales_pca, y_ph, test_size=0.3, random_state=42)

"""----
##Modelado para Brix con PCA
----

En esta sección de Predicción Brix se ha decidido entrenar distintos modelos de Machine Learning para poder seleccionar el que da mejores resultados con nuestros datos en la predicción del Brix.

**Regresión Lineal**
"""

# Inicializar el modelo para Brix
lin_reg_pca = LinearRegression()

# Ajustar el modelo para Brix
lin_reg_pca.fit(X_train_brix_pca_t, y_train_brix_pca_t)

# Realizar predicciones para Brix
y_pred_lin_pca_t = lin_reg_pca.predict(X_test_brix_pca_t)

# Evaluar el modelo para Brix
mse_lin_pca_t = mean_squared_error(y_test_brix_pca_t, y_pred_lin_pca_t)
mae_lin_pca_t = mean_absolute_error(y_test_brix_pca_t, y_pred_lin_pca_t)
r2_lin_pca_t = r2_score(y_test_brix_pca_t, y_pred_lin_pca_t)

print("Regresión Lineal para Brix con PCA")
print(f"Mean Squared Error (MSE): {mse_lin_pca_t}")
print(f"Mean Absolute Error (MAE): {mae_lin_pca_t}")
print(f"R-squared: {r2_lin_pca_t}\n")

"""**Árbol de Decisión**"""

# Inicializar el modelo
tree_reg_pca = DecisionTreeRegressor()

# Ajustar el modelo
tree_reg_pca.fit(X_train_brix_pca_t, y_train_brix_pca_t)

# Realizar predicciones
y_pred_tree_pca_t = tree_reg_pca.predict(X_test_brix_pca_t)

# Evaluar el modelo
mse_tree_pca_t = mean_squared_error(y_test_brix_pca_t, y_pred_tree_pca_t)
mae_tree_pca_t = mean_absolute_error(y_test_brix_pca_t, y_pred_tree_pca_t)
r2_tree_pca_t = r2_score(y_test_brix_pca_t, y_pred_tree_pca_t)

print("Árbol de Decisión para Brix con PCA")
print(f"Mean Squared Error (MSE): {mse_tree_pca_t}")
print(f"Mean Absolute Error (MAE): {mae_tree_pca_t}")
print(f"R-squared: {r2_tree_pca_t}\n")

"""**Random Forest**"""

# Inicializar el modelo
rf_reg_pca = RandomForestRegressor()

# Ajustar el modelo
rf_reg_pca.fit(X_train_brix_pca_t, y_train_brix_pca_t)

# Realizar predicciones
y_pred_rf_pca_t = rf_reg_pca.predict(X_test_brix_pca_t)

# Evaluar el modelo
mse_rf_pca_t = mean_squared_error(y_test_brix_pca_t, y_pred_rf_pca_t)
mae_rf_pca_t = mean_absolute_error(y_test_brix_pca_t, y_pred_rf_pca_t)
r2_rf_pca_t = r2_score(y_test_brix_pca_t, y_pred_rf_pca_t)

print("Random Forest para Brix con PCA")
print(f"Mean Squared Error (MSE): {mse_rf_pca_t}")
print(f"Mean Absolute Error (MAE): {mae_rf_pca_t}")
print(f"R-squared: {r2_rf_pca_t}\n")

"""###Evaluación Modelos Brix con PCA"""

# Evaluación Modelos Brix con PCA
resultados_pca_t = pd.DataFrame({
    'Modelo': ['Regresión Lineal', 'Árbol de Decisión', 'Random Forest'],
    'MSE': [mse_lin_pca_t, mse_tree_pca_t, mse_rf_pca_t],
    'MAE': [mae_lin_pca_t, mae_tree_pca_t, mae_rf_pca_t],
    'R-squared': [r2_lin_pca_t, r2_tree_pca_t, r2_rf_pca_t]
})

print(resultados_pca_t)

"""----
##Modelado para pH con PCA
----

**Regresión Lineal**
"""

# Inicializar el modelo de regresión lineal
lin_reg_brix_ph = LinearRegression()

# Ajustar el modelo usando los datos de entrenamiento
lin_reg_brix_ph.fit(X_train_ph_pca_t, y_train_ph_pca_t)

# Realizar predicciones para el conjunto de prueba
y_pred_ph_pca_t = lin_reg_brix_ph.predict(X_test_ph_pca_t)

# Evaluar el modelo para predecir pH
mse_ph_lin_pca_t = mean_squared_error(y_test_ph_pca_t, y_pred_ph_pca_t)
mae_ph_lin_pca_t = mean_absolute_error(y_test_ph_pca_t, y_pred_ph_pca_t)
r2_ph_lin_pca_t = r2_score(y_test_ph_pca_t, y_pred_ph_pca_t)

print("Modelo de Regresión Lineal:")
print(f"Mean Squared Error: {mse_ph_lin_pca_t}")
print(f"Mean Absolute Error: {mae_ph_lin_pca_t}")
print(f"R-squared: {r2_ph_lin_pca_t}\n")

"""**Árbol de Decisión**"""

# Inicializar el modelo de Árbol de Decisión
tree_reg_brix_ph = DecisionTreeRegressor(random_state=42)

# Ajustar el modelo usando los datos de entrenamiento (Brix como entrada, pH como salida)
tree_reg_brix_ph.fit(X_train_ph_pca_t, y_train_ph_pca_t)

# Realizar predicciones para el conjunto de prueba
y_pred_ph_tree_pca_t = tree_reg_brix_ph.predict(X_test_ph_pca_t)

# Evaluar el modelo para predecir pH
mse_ph_tree_pca_t = mean_squared_error(y_test_ph_pca_t, y_pred_ph_tree_pca_t)
mae_ph_tree_pca_t = mean_absolute_error(y_test_ph_pca_t, y_pred_ph_tree_pca_t)
r2_ph_tree_pca_t = r2_score(y_test_ph_pca_t, y_pred_ph_tree_pca_t)

print("Árbol de Decisión:")
print(f"Mean Squared Error: {mse_ph_tree_pca_t}")
print(f"Mean Absolute Error: {mae_ph_tree_pca_t}")
print(f"R-squared: {r2_ph_tree_pca_t}\n")

"""**Random Forest**"""

# Inicializar el modelo de Random Forest
rf_reg_brix_ph = RandomForestRegressor(n_estimators=100, random_state=42)

# Ajustar el modelo usando los datos de entrenamiento
rf_reg_brix_ph.fit(X_train_ph_pca_t, y_train_ph_pca_t)

# Realizar predicciones para el conjunto de prueba
y_pred_ph_rf_pca_t = rf_reg_brix_ph.predict(X_test_ph_pca_t)

# Evaluar el modelo para predecir pH
mse_ph_rf_pca_t = mean_squared_error(y_test_ph_pca_t, y_pred_ph_rf_pca_t)
mae_ph_rf_pca_t = mean_absolute_error(y_test_ph_pca_t, y_pred_ph_rf_pca_t)
r2_ph_rf_pca_t = r2_score(y_test_ph_pca_t, y_pred_ph_rf_pca_t)

print("Random Forest:")
print(f"Mean Squared Error: {mse_ph_rf_pca_t}")
print(f"Mean Absolute Error: {mae_ph_rf_pca_t}")
print(f"R-squared: {r2_ph_rf_pca_t}\n")

"""###Evaluación Modelos pH con PCA"""

#Evaluación Modelos pH con PCA
resultados_t = pd.DataFrame({
    'Modelo': ['Regresión Lineal', 'Árbol de Decisión', 'Random Forest'],
    'MSE': [mse_ph_lin_pca_t, mse_ph_tree_pca_t, mse_ph_rf_pca_t],
    'MAE': [mae_ph_lin_pca_t, mae_ph_tree_pca_t, mae_ph_rf_pca_t],
    'R-squared': [r2_ph_lin_pca_t, r2_ph_tree_pca_t, r2_ph_rf_pca_t]
})

print(resultados_t)

"""----
#Cuarta iteración
----

Se realiza el pretratamiento PCA específico para el pH y se le agregará a las variables de entrada el Brix.

---
##Preparación de Datos PCA para pH
---

Se realiza los siguientes pasos de pretratamiento para realizar un PCA

###Suavizado
"""

# Suavizar los datos espectrales
window_length = 9  # Impar
polyorder = 3      # Grado del polinomio para el suavizado
espectros_suavizados = savgol_filter(espectros, window_length=window_length, polyorder=polyorder, axis=1)

# Convertir a DataFrame para facilitar la manipulación
espectros_suavizados_df = pd.DataFrame(espectros_suavizados, columns=numeros_onda)

fila_a_graficar = 0

plt.figure(figsize=(12, 6))

# Graficar el espectro original filtrado
plt.plot(numeros_onda_filtrados_1, espectros_filtrados_1.iloc[fila_a_graficar, :],
         label='Original', color='red', alpha=0.5)

# Graficar el espectro suavizado y filtrado
plt.plot(numeros_onda_filtrados_1, espectros_suavizados_df.loc[fila_a_graficar, rango_espectros_1],
         label='Suavizado', linewidth=2, color='blue')

# Configuraciones del gráfico
plt.title(f'Comparación de Espectro Original y Suavizado - Fila {fila_a_graficar + 1}')
plt.xlabel('Números de onda (cm-1)')
plt.ylabel('Intensidad')
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

"""###Segunda Derivada"""

# Aplicar la primera derivada al espectro suavizado
derivada = savgol_filter(espectros_suavizados, window_length=window_length, polyorder=polyorder, deriv=2, axis=1)

# Convertir la derivada a DataFrame para facilitar la manipulación
derivada_df = pd.DataFrame(derivada, columns=numeros_onda)

# Fila a graficar
fila_a_graficar = 0

# Graficar el espectro original, suavizado y la derivada
plt.figure(figsize=(12, 6))

# Graficar el espectro suavizado
plt.plot(numeros_onda, espectros_suavizados_df.iloc[fila_a_graficar, :], label='Suavizado', linewidth=2, color='blue')

# Graficar la primera derivada
plt.plot(numeros_onda, derivada_1_df.iloc[fila_a_graficar, :], label='Segunda Derivada', linewidth=2, color='green')

# Configuración del gráfico
plt.title(f'Comparación: Espectro Suavizado y Segunda Derivada')
plt.xlabel('Números de onda (cm-1)')
plt.ylabel('Intensidad')
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

"""###SNV"""

# Seleccionar el índice del espectro que deseas graficar
indice_espectro = 0

# Extraer el espectro original (sin SNV)
espectro_original = derivada_df.iloc[indice_espectro, :]

# Aplicar SNV a los datos
espectros_snv = (derivada_df - derivada_df.mean(axis=1).values.reshape(-1, 1)) / derivada_df.std(axis=1).values.reshape(-1, 1)

# Extraer el espectro después de aplicar SNV
espectro_snv = espectros_snv.iloc[indice_espectro, :]

# Graficar el espectro original y el espectro después de SNV
plt.figure(figsize=(12, 6))

# Graficar el espectro original
plt.plot(derivada_df.columns, espectro_original, label='Espectro con Derivada', color='blue', alpha=0.7)

# Graficar el espectro después de SNV
plt.plot(derivada_df.columns, espectro_snv, label='Espectro con SNV', color='green', alpha=0.7)

# Configuraciones del gráfico
plt.title(f'Comparación de Espectro Original y con SNV - Índice {indice_espectro}')
plt.xlabel('Número de Onda (cm⁻¹)')
plt.ylabel('Intensidad')
plt.legend()
plt.grid(True)

# Mostrar el gráfico
plt.show()

"""###Análisis de Componentes Principales (PCA)"""

# Definir el número de componentes principales
n_componentes = 2

# Crear una instancia de PCA y ajustar los datos normalizados
pca = PCA(n_components=n_componentes)
componentes_principales_ph = pca.fit_transform(espectros_snv)

# Crear un DataFrame con los componentes principales para facilitar el manejo
pca_df = pd.DataFrame(componentes_principales_ph, columns=[f'Componente Principal {i+1}' for i in range(n_componentes)])

# Graficar los componentes principales en 2D
plt.figure(figsize=(10, 6))
plt.scatter(pca_df['Componente Principal 1'], pca_df['Componente Principal 2'], alpha=0.7, color='blue')
plt.title('PCA - Componentes Principales en 2D')
plt.xlabel('Componente Principal 1')
plt.ylabel('Componente Principal 2')
plt.grid(True)

# Mostrar el gráfico
plt.show()

# Imprimir la varianza explicada por cada componente
print("Varianza explicada por cada componente:", pca.explained_variance_ratio_)

"""---
## Entrenamiento pH con variable de entrada Brix y PCA
---

Desde el resultado del PCA de "componentes_principales", se desarrolla un nuevo enfoque de entrenamiento para la predicción del Brix
"""

# Crear un DataFrame con los componentes principales
df_componentes_principales_brix = pd.DataFrame(componentes_principales_ph, columns=['Componente 1', 'Componente 2'])

# Asegurarse de que matriz_x tenga un índice correcto
matriz_x.reset_index(drop=True, inplace=True)  # Reiniciar el índice si es necesario

# Agregar las columnas 'Días' y 'Tipo de uva' a los componentes principales
df_componentes_principales_brix['Días'] = matriz_x['Días'].reset_index(drop=True)  # Asegurarse de que las longitudes coincidan
df_componentes_principales_brix['Tipo de uva'] = matriz_x['Tipo de uva'].reset_index(drop=True)  # Asegurarse de que las longitudes coincidan

# Separar 'Tipo de uva' en dos nuevas columnas 'Racimo' y 'Viña'
df_componentes_principales_brix['Viña'] = df_componentes_principales_brix['Tipo de uva'].str[1]  # Segundo carácter
df_componentes_principales_brix['Racimo'] = df_componentes_principales_brix['Tipo de uva'].str[2]  # Tercer carácter

# Eliminar la columna 'Tipo de uva'
df_componentes_principales_brix.drop(columns=['Tipo de uva'], inplace=True)

# Agregar la columna de Brix
df_componentes_principales_brix['Brix'] = matriz_y['Brix'].reset_index(drop=True)  # Asegúrate de que la longitud coincida

# Normalizar la columna de Brix
scaler_brix = StandardScaler()
df_componentes_principales_brix[['Brix']] = scaler_brix.fit_transform(df_componentes_principales_brix[['Brix']])

# Codificación de variables categóricas ('Viña' y 'Racimo')
df_componentes_principales_brix = pd.get_dummies(df_componentes_principales_brix, columns=['Viña', 'Racimo'], drop_first=False)

# Normalizar la columna 'Días'
scaler_dias = StandardScaler()
df_componentes_principales_brix[['Días']] = scaler_dias.fit_transform(df_componentes_principales_brix[['Días']])

# Mostrar el DataFrame resultante
print(df_componentes_principales_brix.head())

"""Al aplicar pd.get_dummies() para convertir la columna Tipo de uva en variables dummy, la columna original se convierte en varias columnas binarias, cada una representando la posición según la viña y del racimo, siendo A=Alto, B=Bajo y M=Medio.

Se realiza la separación de los datos para el entrenamiento y el test en un 70% y 30% ya que es el que mejor resultados da.
"""

# Separar en conjunto de entrenamiento para pH
y_ph_b = matriz_y.iloc[:, 1].values

# Usar el DataFrame con los componentes principales
X_train_b, X_test_b, y_train_b, y_test_b = train_test_split(df_componentes_principales_brix, y_ph_b, test_size=0.3, random_state=42)

"""----
##Modelado para pH con Brix y PCA
----

**Regresión Lineal**
"""

# Inicializar el modelo de regresión lineal
lin_reg_brix_b = LinearRegression()

# Ajustar el modelo usando los datos de entrenamiento
lin_reg_brix_b.fit(X_train_b, y_train_b)

# Realizar predicciones para el conjunto de prueba
y_pred_b = lin_reg_brix_b.predict(X_test_b)

# Evaluar el modelo para predecir pH
mse_ph_lin_b= mean_squared_error(y_test_b, y_pred_b)
mae_ph_lin_b = mean_absolute_error(y_test_b, y_pred_b)
r2_ph_lin_b = r2_score(y_test_b, y_pred_b)

print("Modelo de Regresión Lineal:")
print(f"Mean Squared Error: {mse_ph_lin_b}")
print(f"Mean Absolute Error: {mae_ph_lin_b}")
print(f"R-squared: {r2_ph_lin_b}\n")

"""**Árbol de Decisión**"""

# Inicializar el modelo de Árbol de Decisión
tree_reg_brix_ph = DecisionTreeRegressor(random_state=42)

# Ajustar el modelo usando los datos de entrenamiento (Brix como entrada, pH como salida)
tree_reg_brix_ph.fit(X_train_b, y_train_b)

# Realizar predicciones para el conjunto de prueba
y_pred_b = tree_reg_brix_ph.predict(X_test_b)

# Evaluar el modelo para predecir pH
mse_ph_tree_b = mean_squared_error(y_test_b, y_pred_b)
mae_ph_tree_b = mean_absolute_error(y_test_b, y_pred_b)
r2_ph_tree_b = r2_score(y_test_b, y_pred_b)

print("Árbol de Decisión:")
print(f"Mean Squared Error: {mse_ph_tree_b}")
print(f"Mean Absolute Error: {mae_ph_tree_b}")
print(f"R-squared: {r2_ph_tree_b}\n")

"""**Random Forest**"""

# Inicializar el modelo de Random Forest
rf_reg_brix_ph = RandomForestRegressor(n_estimators=100, random_state=42)

# Ajustar el modelo usando los datos de entrenamiento
rf_reg_brix_ph.fit(X_train_b, y_train_b)

# Realizar predicciones para el conjunto de prueba
y_pred_b = rf_reg_brix_ph.predict(X_test_b)

# Evaluar el modelo para predecir pH
mse_ph_rf_b = mean_squared_error(y_test_b, y_pred_b)
mae_ph_rf_b = mean_absolute_error(y_test_b, y_pred_b)
r2_ph_rf_b = r2_score(y_test_b, y_pred_b)

print("Random Forest:")
print(f"Mean Squared Error: {mse_ph_rf_b}")
print(f"Mean Absolute Error: {mae_ph_rf_b}")
print(f"R-squared: {r2_ph_rf_b}\n")

"""###Evaluación Modelos pH con Brix y PCA"""

#Evaluación Modelos pH con Brix y PCA
resultados_b = pd.DataFrame({
    'Modelo': ['Regresión Lineal', 'Árbol de Decisión', 'Random Forest'],
    'MSE': [mse_ph_lin_b, mse_ph_tree_b, mse_ph_rf_b],
    'MAE': [mae_ph_lin_b, mae_ph_tree_b, mae_ph_rf_b],
    'R-squared': [r2_ph_lin_b, r2_ph_tree_b, r2_ph_rf_b]
})

print(resultados_b)

"""---
#**Resultados**
---

##Análisis Precciones Brix
"""

# Evaluación Modelos Brix con PCA (Iteración 1)
resultados_pca_t = pd.DataFrame({
    'Modelo': ['Regresión Lineal', 'Árbol de Decisión', 'Random Forest'],
    'MSE': [mse_lin_pca_t, mse_tree_pca_t, mse_rf_pca_t],
    'MAE': [mae_lin_pca_t, mae_tree_pca_t, mae_rf_pca_t],
    'R-squared': [r2_lin_pca_t, r2_tree_pca_t, r2_rf_pca_t]
})

# Evaluación Modelos Brix con PCA sin tratamiento (Iteración 2)
resultados_pca = pd.DataFrame({
    'Modelo': ['Regresión Lineal', 'Árbol de Decisión', 'Random Forest'],
    'MSE': [mse_lin_pca, mse_tree_pca, mse_rf_pca],
    'MAE': [mae_lin_pca, mae_tree_pca, mae_rf_pca],
    'R-squared': [r2_lin_pca, r2_tree_pca, r2_rf_pca]
})

# Evaluación Modelos Brix (Iteración 3)
resultados = pd.DataFrame({
    'Modelo': ['Regresión Lineal', 'Árbol de Decisión', 'Random Forest'],
    'MSE': [mse_lin, mse_tree, mse_rf],
    'MAE': [mae_lin, mae_tree, mae_rf],
    'R-squared': [r2_lin, r2_tree, r2_rf]
})

# Añadir una columna que indica la iteración
resultados_pca_t['Iteración'] = 'PCA con Tratamiento'
resultados_pca['Iteración'] = 'PCA sin Tratamiento'
resultados['Iteración'] = 'Sin PCA'

# Concatenar los tres dataframes
resultados_completos = pd.concat([resultados, resultados_pca, resultados_pca_t], ignore_index=True)

# Reestructuración para crear subcolumnas (Métrica por Iteración)
resultados_completos = pd.melt(resultados_completos, id_vars=['Modelo', 'Iteración'], value_vars=['MSE', 'MAE', 'R-squared'],
                               var_name='Métrica', value_name='Valor')

# Pivotar la tabla para que las métricas estén como subcolumnas bajo cada iteración
resultados_pivot = resultados_completos.pivot_table(index=['Modelo'], columns=['Iteración', 'Métrica'], values='Valor')

# Mostrar la tabla final
display(resultados_pivot)

# Evaluación Modelos Brix con PCA (Iteración 1)
resultados_pca_t = pd.DataFrame({
    'Modelo': ['Regresión Lineal', 'Árbol de Decisión', 'Random Forest'],
    'MSE': [mse_lin_pca_t, mse_tree_pca_t, mse_rf_pca_t],
    'MAE': [mae_lin_pca_t, mae_tree_pca_t, mae_rf_pca_t],
    'R-squared': [r2_lin_pca_t, r2_tree_pca_t, r2_rf_pca_t]
})

# Evaluación Modelos Brix con PCA sin tratamiento (Iteración 2)
resultados_pca = pd.DataFrame({
    'Modelo': ['Regresión Lineal', 'Árbol de Decisión', 'Random Forest'],
    'MSE': [mse_lin_pca, mse_tree_pca, mse_rf_pca],
    'MAE': [mae_lin_pca, mae_tree_pca, mae_rf_pca],
    'R-squared': [r2_lin_pca, r2_tree_pca, r2_rf_pca]
})

# Evaluación Modelos Brix (Iteración 3)
resultados = pd.DataFrame({
    'Modelo': ['Regresión Lineal', 'Árbol de Decisión', 'Random Forest'],
    'MSE': [mse_lin, mse_tree, mse_rf],
    'MAE': [mae_lin, mae_tree, mae_rf],
    'R-squared': [r2_lin, r2_tree, r2_rf]
})

# Añadir una columna que indica la iteración
resultados_pca_t['Iteración'] = 'PCA con Tratamiento'
resultados_pca['Iteración'] = 'PCA sin Tratamiento'
resultados['Iteración'] = 'Sin PCA'

# Concatenar los tres dataframes
resultados_completos = pd.concat([resultados, resultados_pca, resultados_pca_t], ignore_index=True)

# Reestructuración para crear subcolumnas (Métrica por Iteración)
resultados_completos = pd.melt(resultados_completos, id_vars=['Modelo', 'Iteración'], value_vars=['R-squared'],
                               var_name='Métrica', value_name='Valor')

# Pivotar la tabla para que las métricas estén como subcolumnas bajo cada iteración
resultados_pivot = resultados_completos.pivot_table(index=['Modelo'], columns=['Iteración', 'Métrica'], values='Valor')

# Mostrar la tabla final
display(resultados_pivot)

"""##Análisis Precciones pH"""

# Evaluación Modelos pH con Brix y PCA (Iteración 4)
resultados_b = pd.DataFrame({
    'Modelo': ['Regresión Lineal', 'Árbol de Decisión', 'Random Forest'],
    'MSE': [mse_ph_lin_b, mse_ph_tree_b, mse_ph_rf_b],
    'MAE': [mae_ph_lin_b, mae_ph_tree_b, mae_ph_rf_b],
    'R-squared': [r2_ph_lin_b, r2_ph_tree_b, r2_ph_rf_b]
})

# Evaluación Modelos pH con PCA (Iteración 3)
resultados_t = pd.DataFrame({
    'Modelo': ['Regresión Lineal', 'Árbol de Decisión', 'Random Forest'],
    'MSE': [mse_ph_lin_pca_t, mse_ph_tree_pca_t, mse_ph_rf_pca_t],
    'MAE': [mae_ph_lin_pca_t, mae_ph_tree_pca_t, mae_ph_rf_pca_t],
    'R-squared': [r2_ph_lin_pca_t, r2_ph_tree_pca_t, r2_ph_rf_pca_t]
})

# Evaluación Modelos pH con PCA sin tratamiento (Iteración 2)
resultados = pd.DataFrame({
    'Modelo': ['Regresión Lineal', 'Árbol de Decisión', 'Random Forest'],
    'MSE': [mse_ph_lin_pca, mse_ph_tree_pca, mse_ph_rf_pca],
    'MAE': [mae_ph_lin_pca, mae_ph_tree_pca, mae_ph_rf_pca],
    'R-squared': [r2_ph_lin_pca, r2_ph_tree_pca, r2_ph_rf_pca]
})

# Evaluación Modelos pH (Iteración 1)
resultados_4 = pd.DataFrame({
    'Modelo': ['Regresión Lineal', 'Árbol de Decisión', 'Random Forest'],
    'MSE': [mse_ph_lin, mse_ph_tree, mse_ph_rf],
    'MAE': [mae_ph_lin, mae_ph_tree, mae_ph_rf],
    'R-squared': [r2_ph_lin, r2_ph_tree, r2_ph_rf]
})

# Añadir una columna que indica la iteración
resultados_b['Iteración'] = 'Brix y PCA'
resultados_t['Iteración'] = 'PCA con Tratamiento'
resultados['Iteración'] = 'PCA sin Tratamiento'
resultados_4['Iteración'] = 'Sin PCA'

# Concatenar los cuatro dataframes
resultados_completos_ph = pd.concat([resultados_b, resultados_t, resultados, resultados_4], ignore_index=True)

# Reestructuración para crear subcolumnas (Métrica por Iteración)
resultados_completos_ph = pd.melt(resultados_completos_ph, id_vars=['Modelo', 'Iteración'], value_vars=['MSE', 'MAE', 'R-squared'],
                                  var_name='Métrica', value_name='Valor')

# Pivotar la tabla para que las métricas estén como subcolumnas bajo cada iteración
resultados_pivot_ph = resultados_completos_ph.pivot_table(index=['Modelo'], columns=['Iteración', 'Métrica'], values='Valor')

# Mostrar la tabla final
display(resultados_pivot_ph)

# Evaluación Modelos pH con Brix y PCA (Iteración 4)
resultados_b = pd.DataFrame({
    'Modelo': ['Regresión Lineal', 'Árbol de Decisión', 'Random Forest'],
    'MSE': [mse_ph_lin_b, mse_ph_tree_b, mse_ph_rf_b],
    'MAE': [mae_ph_lin_b, mae_ph_tree_b, mae_ph_rf_b],
    'R-squared': [r2_ph_lin_b, r2_ph_tree_b, r2_ph_rf_b]
})

# Evaluación Modelos pH con PCA (Iteración 3)
resultados_t = pd.DataFrame({
    'Modelo': ['Regresión Lineal', 'Árbol de Decisión', 'Random Forest'],
    'MSE': [mse_ph_lin_pca_t, mse_ph_tree_pca_t, mse_ph_rf_pca_t],
    'MAE': [mae_ph_lin_pca_t, mae_ph_tree_pca_t, mae_ph_rf_pca_t],
    'R-squared': [r2_ph_lin_pca_t, r2_ph_tree_pca_t, r2_ph_rf_pca_t]
})

# Evaluación Modelos pH con PCA sin tratamiento (Iteración 2)
resultados = pd.DataFrame({
    'Modelo': ['Regresión Lineal', 'Árbol de Decisión', 'Random Forest'],
    'MSE': [mse_ph_lin_pca, mse_ph_tree_pca, mse_ph_rf_pca],
    'MAE': [mae_ph_lin_pca, mae_ph_tree_pca, mae_ph_rf_pca],
    'R-squared': [r2_ph_lin_pca, r2_ph_tree_pca, r2_ph_rf_pca]
})

# Evaluación Modelos pH (Iteración 1)
resultados_4 = pd.DataFrame({
    'Modelo': ['Regresión Lineal', 'Árbol de Decisión', 'Random Forest'],
    'MSE': [mse_ph_lin, mse_ph_tree, mse_ph_rf],
    'MAE': [mae_ph_lin, mae_ph_tree, mae_ph_rf],
    'R-squared': [r2_ph_lin, r2_ph_tree, r2_ph_rf]
})

# Añadir una columna que indica la iteración
resultados_b['Iteración'] = 'Brix y PCA'
resultados_t['Iteración'] = 'PCA con Tratamiento'
resultados['Iteración'] = 'PCA sin Tratamiento'
resultados_4['Iteración'] = 'Sin PCA'

# Concatenar los cuatro dataframes
resultados_completos_ph = pd.concat([resultados_b, resultados_t, resultados, resultados_4], ignore_index=True)

# Reestructuración para crear subcolumnas (Métrica por Iteración)
resultados_completos_ph = pd.melt(resultados_completos_ph, id_vars=['Modelo', 'Iteración'], value_vars=['R-squared'],
                                  var_name='Métrica', value_name='Valor')

# Pivotar la tabla para que las métricas estén como subcolumnas bajo cada iteración
resultados_pivot_ph = resultados_completos_ph.pivot_table(index=['Modelo'], columns=['Iteración', 'Métrica'], values='Valor')

# Mostrar la tabla final
display(resultados_pivot_ph)